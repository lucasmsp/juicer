#!/usr/bin/python
# -*- coding: utf-8 -*-
"""
Auto-generated Multiplatform (Scikit-Learn/Spark) code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided
"""

import datetime
import decimal
import functools
#import pyarrow as pa
from pyarrow import fs as hdfs
import os
import re
import json
import simplejson
import string
import sys
import time
import unicodedata
import numpy as np
# import modin.pandas as pd
import functools
import threading


from concurrent.futures import ThreadPoolExecutor
from timeit import default_timer as timer
#from juicer.scikit_learn.model_operation import ModelsEvaluationResultList
from juicer.spark.reports import *


from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.sql import functions, types, Row, DataFrame
from pyspark.sql.utils import IllegalArgumentException
from pyspark.sql.window import Window
from pyspark.ml.linalg import Vectors, VectorUDT

from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *
from pyspark.ml.regression import *
from pyspark.mllib.evaluation import *
from juicer import privaaas
from juicer.util import dataframe_util, get_emitter
from juicer.spark.reports import *
from juicer.spark.util import assemble_features_pipeline_model
#from juicer.spark.ml_operation import ModelsEvaluationResultList
from juicer.spark.custom_library import *

import pandas as pd
pd.DataFrame.iteritems = pd.DataFrame.items
# FIX TO USE PANDAS 2.0 with Spark 3.1.5rc

import traceback

{% autopep8 %}

{%- for imps in transpiler.imports %}
{{imps}}
{%- endfor %}

{%- if transpiler.custom_functions %}
# Custom functions
{%- for code in transpiler.custom_functions.values() %}
{{code}}
{%- endfor -%}
# End custom functions
{%- endif %}

def sklearn_logging(msg):
    print(msg)

def migrate_data(task_id, data, target_platform, spark_session, emit_event, parent_id):
    if (target_platform == 4) and hasattr(data, 'toPandas'):
        data = data.toPandas()
        emit_event(name='update task', message=_('Data of parent "{}" was converted from Spark\'s DataFrame to Pandas.'.format(parent_id)),
                   identifier=task_id, status='RUNNING')
    elif (target_platform == 1) and isinstance(data, pd.DataFrame):
        data = spark_session.createDataFrame(data)
        emit_event(name='update task', message=_('Data of parent "{}" was converted from Pandas\'s DataFrame to Spark.'.format(parent_id)),
                   identifier=task_id, status='RUNNING')
    return data

executor = ThreadPoolExecutor(max_workers=3*{{instances|length}})
submission_lock = threading.Lock()
task_futures = {}

{%- for instance in instances %}
{%-  handleinstance instance %}
{%- if instance.has_code and instance.enabled %}
{%- set task = instance.parameters.task %}
{%- set task_id = task.id %}


# noinspection PyUnusedLocal
def {{task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event):
    """
    {%- if task.forms.comment and task.forms.comment.value %}
    {{task.forms.comment.value.strip().replace('"', '')}}
    {%- else %}
    Operation {{task_id }}
    {%- endif %}
    Task hash: {{instance.parameters.hash}}.
    """
    task_id = '{{task_id}}'
    {%- set target_platform = instance.parameters['multiplatform']['target'] | int    %}
    target_platform = {{ target_platform }}
    {%- if task.parents %}
    {% set msg = dict_msgs['lemonade_task_afterbefore'].format(instance.parameters.task.parents, instance.parameters.task.id) %}
    sklearn_logging("{{msg}}")

    # If the task's result is not cached, we submit its dependencies first
    {%- set parents = transpiler._get_parent_tasks(instances_by_task_id, instance) %}
    {%- for parent_id, method in parents %}
    parent_id = '{{parent_id}}'
    submission_lock.acquire()
    if parent_id not in task_futures:
        task_futures[parent_id] = executor.submit(
                lambda: {{method}}(spark_session, cached_state, emit_event))
    submission_lock.release()
    {%- endfor %}
    {%- endif %}

    {%- if parents %}
    # Next we wait for the dependencies to complete
    {%- for parent_id, method in parents %}
    {%- set parent_instance = instances_by_task_id[parent_id] %}
    {%- if parent_instance.get_output_names(", ") %}
    parent_result = task_futures['{{parent_id}}'].result()
    {%- for port_name,out in zip(parent_instance.parameters.task.port_names, parent_instance.get_output_names(',').split(','))%}
    {{out}} = parent_result['{{port_name}}']
    # --- Begin data migration --- #
    {{out}} = migrate_data(task_id, {{out}}, target_platform, spark_session, emit_event, '{{parent_id}}')
    # --- End data migration --- #
    {%- endfor %}
    ts_{{parent_instance.output}} = parent_result['time']
    {% endif %}
    {%- endfor %}
    {% set msg = dict_msgs['lemonade_task_parents'] % instance.parameters.task.id %}
    sklearn_logging("{{msg}}")
    {%- endif %}

    {%- if not plain %}
    emit_task_running(task_id, spark_session, emit_event)
    {%- if target_platform == 1 %}
    emit_event(name='update task', message=_('Operation will be executed in Spark.'),
               identifier=task_id, status='RUNNING')
    {%- elif target_platform == 4 %}
    emit_event(name='update task', message=_('Operation will be executed in Pandas/Sklearn.'),
               identifier=task_id, status='RUNNING')
    {%- endif %}
    {%- endif %}
    start = timer()
    # --- Begin operation code ---- #
    {{instance.generate_code().strip() | indent(width=4, first=False)}}
    # --- End operation code ---- #

    {%- if not plain %}
    {%- for gen_result in instance.get_generated_results() %}
    emit_event(name='task result', message=_('{{gen_result.type}}'),
               status='COMPLETED',
               identifier='{{task.operation.id}}/{{task_id}}')
    {%- endfor %}
    {%- endif %}

    results = {
        'task_name': '{{task.name}}',
      {%- set is_leaf = instance.out_degree == 0 %}
      {%- for port_name,out in zip(task.port_names, instance.get_output_names(',').split(',')) %}
        {%- if port_name and out %}
         '{{port_name}}': {{out}},
        {%- endif %}
      {%- endfor %}
    }

    {%- if target_platform  == 4 %}
    {%- if instance.contains_results() %}
    outputs = [(name, out) for name, out in results.items() if isinstance(out, pd.DataFrame)]
    {%- if instance.has_code and instance.enabled and instance.contains_sample %}
    for name, out in outputs:
        dataframe_util.emit_sample_sklearn(task_id, out, emit_event, name)
    {%- endif %}
    {%- if instance.has_code and instance.enabled and instance.contains_schema %}
    for name, out in outputs:
        dataframe_util.emit_schema_sklearn(task_id, out, emit_event, name)
        n_rows = len(out)
        emit_event(name='update task', message=_('Records in this output: '+str(n_rows)),
               identifier=task_id, status='RUNNING')
    {%- endif %}
    {%- endif %}
    {%- elif target_platform  == 1 %}
    {%- if instance.contains_results() %}
    df_types = (DataFrame, dataframe_util.LazySparkTransformationDataframe)
    outputs = [(name, out) for name, out in results.items()
        if isinstance(out, df_types)]
    {%- if instance.has_code and instance.enabled and instance.contains_sample %}
    for name, out in outputs:
        dataframe_util.emit_sample(task_id, out, emit_event, name)
    {%- endif %}
    {%- if instance.has_code and instance.enabled and instance.contains_schema %}
    for name, out in outputs:
        n_rows = out.count()
        emit_event(name='update task', message=_('Records in this output: '+str(n_rows)),
               identifier=task_id, status='RUNNING')
        dataframe_util.emit_schema(task_id, out, emit_event, name)
    {%- endif %}
    {%- endif %}
    {%- endif %}

    {%- if not plain %}
    emit_task_completed(task_id, spark_session, emit_event)
    {%- endif %}

    results['time'] = timer() - start
    return results


{%- endif %}
{%- endhandleinstance %}
{% endfor %}

def emit_task_running(task_id, spark_session, emit_event):
    emit_event(name='update task', message=_('Task running'), status='RUNNING',
               identifier=task_id)

def emit_task_completed(task_id, spark_session, emit_event):
    emit_event(name='update task', message=_('Task completed'),
               status='COMPLETED', identifier=task_id)

def get_results(_task_futures, task_id):
    return _task_futures[task_id].result() if task_id in _task_futures else None

def main(spark_session, cached_state, emit_event):
    """ Run generated code """
    print(spark_session.conf.get("spark.sql.files.minPartitionNum"))
    try:
        {%- for instance in instances %}
        {%- if instance.has_code and instance.enabled and instance.multiple_inputs %}
        {{instance.get_inputs_names.replace(',', '=') }} = None
        {%- endif %}
        {%- endfor %}

        {%- set ids_and_methods = transpiler.get_ids_and_methods(instances) %}
        {%- for task_id, method in ids_and_methods.items() %}
        task_futures['{{task_id}}'] = executor.submit(
            lambda: {{method}}(spark_session, cached_state, emit_event))
        {%- endfor %}

        {%- for task_id in ids_and_methods.keys() %}
        {%- set s = dependency_controller.satisfied(task_id) %}
        task_futures['{{task_id}}'].result()
        {%- endfor %}
        {%- for disabled in transpiler.get_disabled_tasks(instances) %}
        emit_event(name='update task', message=_(
            'Task completed, but not executed (not used in the workflow).'),
            status='COMPLETED',
            identifier='{{disabled.parameters.task.id}}')
        {%- endfor %}

        {%- for disabled in disabled_tasks %}
        emit_event(name='update task', message=_(
            'Task completed, but not executed (not used in the workflow).'),
            status='COMPLETED', identifier='{{disabled}}')
        {%- endfor %}

        return {
            'status': 'OK',
            'message': 'Execution defined',
            {%- for instance in transpiler._get_enabled_tasks(instances) %}
            '{{instance.parameters.task.id}}':
                [get_results(task_futures,
                '{{instance.parameters.task.id}}'),
                '{{instance.parameters.hash}}'],
            {%- endfor %}
        }
    except Exception as e:
        traceback.print_exc(file=sys.stderr)
        raise

{%- if execute_main %}

def dummy_emit_event(room, namespace):
    def _dummy_emit_event(name, message, status, identifier, **kwargs):
        return None
    return _dummy_emit_event

from pyspark.sql import SparkSession
spark_session = SparkSession.builder.getOrCreate()
spark_session.sparkContext.setLogLevel('INFO')
#TO DO: LOGLEVEL
main(spark_session, {}, dummy_emit_event(room=-1, namespace='/none'))

{%- endif %}

{% endautopep8 %}
